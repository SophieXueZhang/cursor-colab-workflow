#!/usr/bin/env python3
"""
ç¤ºä¾‹GPUåŠ é€Ÿçš„æœºå™¨å­¦ä¹ è„šæœ¬
åœ¨Google Colabä¸­ä½¿ç”¨GPUè¿è¡Œ
"""

import torch
import torch.nn as nn
import torch.optim as optim
import numpy as np
import matplotlib.pyplot as plt
from datetime import datetime

def check_gpu():
    """æ£€æŸ¥GPUå¯ç”¨æ€§"""
    if torch.cuda.is_available():
        device = torch.device("cuda")
        print(f"âœ… GPUå¯ç”¨: {torch.cuda.get_device_name(0)}")
        print(f"GPUå†…å­˜: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f}GB")
    else:
        device = torch.device("cpu")
        print("âš ï¸  GPUä¸å¯ç”¨ï¼Œä½¿ç”¨CPU")
    return device

class SimpleNN(nn.Module):
    """ç®€å•çš„ç¥ç»ç½‘ç»œç¤ºä¾‹"""
    def __init__(self, input_size=784, hidden_size=128, num_classes=10):
        super(SimpleNN, self).__init__()
        self.fc1 = nn.Linear(input_size, hidden_size)
        self.relu = nn.ReLU()
        self.fc2 = nn.Linear(hidden_size, hidden_size)
        self.fc3 = nn.Linear(hidden_size, num_classes)
        self.dropout = nn.Dropout(0.2)
        
    def forward(self, x):
        x = x.view(-1, 784)  # å±•å¹³è¾“å…¥
        x = self.fc1(x)
        x = self.relu(x)
        x = self.dropout(x)
        x = self.fc2(x)
        x = self.relu(x)
        x = self.dropout(x)
        x = self.fc3(x)
        return x

def train_model(device):
    """è®­ç»ƒæ¨¡å‹ç¤ºä¾‹"""
    print("ğŸš€ å¼€å§‹è®­ç»ƒæ¨¡å‹...")
    
    # åˆ›å»ºæ¨¡æ‹Ÿæ•°æ®
    batch_size = 64
    input_size = 784
    num_classes = 10
    num_epochs = 5
    
    # åˆ›å»ºæ¨¡å‹å¹¶ç§»åŠ¨åˆ°GPU
    model = SimpleNN(input_size, 128, num_classes).to(device)
    criterion = nn.CrossEntropyLoss()
    optimizer = optim.Adam(model.parameters(), lr=0.001)
    
    # æ¨¡æ‹Ÿè®­ç»ƒæ•°æ®
    train_losses = []
    
    for epoch in range(num_epochs):
        epoch_loss = 0
        num_batches = 100
        
        for batch in range(num_batches):
            # ç”Ÿæˆéšæœºæ•°æ®
            data = torch.randn(batch_size, input_size).to(device)
            targets = torch.randint(0, num_classes, (batch_size,)).to(device)
            
            # å‰å‘ä¼ æ’­
            outputs = model(data)
            loss = criterion(outputs, targets)
            
            # åå‘ä¼ æ’­
            optimizer.zero_grad()
            loss.backward()
            optimizer.step()
            
            epoch_loss += loss.item()
        
        avg_loss = epoch_loss / num_batches
        train_losses.append(avg_loss)
        print(f"Epoch [{epoch+1}/{num_epochs}], Loss: {avg_loss:.4f}")
    
    return train_losses

def plot_results(losses):
    """ç»˜åˆ¶è®­ç»ƒç»“æœ"""
    plt.figure(figsize=(10, 6))
    plt.plot(losses, 'b-', linewidth=2)
    plt.title('Training Loss Over Time', fontsize=16)
    plt.xlabel('Epoch', fontsize=12)
    plt.ylabel('Loss', fontsize=12)
    plt.grid(True, alpha=0.3)
    plt.show()

def main():
    """ä¸»å‡½æ•°"""
    print("=" * 50)
    print("ğŸ¯ Cursor + Colab + GitHub å·¥ä½œæµç¤ºä¾‹")
    print("=" * 50)
    print(f"è¿è¡Œæ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    
    # æ£€æŸ¥GPU
    device = check_gpu()
    
    # è®­ç»ƒæ¨¡å‹
    losses = train_model(device)
    
    # ç»˜åˆ¶ç»“æœ
    plot_results(losses)
    
    print("âœ… è®­ç»ƒå®Œæˆï¼")
    print("ğŸ’¡ æç¤º: ä¿®æ”¹ä»£ç åï¼Œåœ¨Cursorä¸­æ¨é€åˆ°GitHubï¼Œç„¶ååœ¨Colabä¸­è¿è¡Œ !git pull æ‹‰å–æœ€æ–°ä»£ç ")

if __name__ == "__main__":
    main() 